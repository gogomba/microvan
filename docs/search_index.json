[["index.html", "Microvan About", " Microvan Hammond 2022-05-28 About The case describes a project by Grosse Pointe Associates (GPA) investigating the potential market for “microvans”. The objectives of this case are: 1. To give you experience segmenting customers in a market 2. To have you determine which segment(s) would be good to target based on analysis of the data. 3. To have you relate the segment(s) to demographic variables potentially useful for targeting "],["executive-summary.html", "Chapter1 Executive Summary", " Chapter1 Executive Summary The case describes a project by Grosse Pointe Associates (GPA) investigating the potential market for “microvans”. The objectives of this case are: 1. To give you experience segmenting customers in a market 2. To have you determine which segment(s) would be good to target based on analysis of the data. 3. To have you relate the segment(s) to demographic variables potentially useful for targeting. "],["explorative-data-analysis.html", "Chapter2 Explorative Data Analysis 2.1 Correlation Analysis", " Chapter2 Explorative Data Analysis # multivariate analysis and basic descriptive statistics library(psych) # data processing library(dplyr) library(tidyverse) library(data.table) # plot library(ggplot2) library(corrplot) # rMarkdown library(rmarkdown) library(kableExtra) # file path management library(here) # stat library(factoextra) # data dataPath = file.path(dataDir, &quot;microvan.csv&quot;) microvan = read.csv(dataPath, sep=&quot;;&quot;) # Convert all integer variables into numeric ones for futher work microvan &lt;- as.data.table(lapply(microvan, as.numeric)) # data description description = psych::describe(microvan) description ## vars n mean sd median trimmed mad min max range skew ## subjnumb 1 400 200.50 115.61 200 200.50 148.3 1 400 399 0.00 ## mvliking 2 400 4.84 2.69 5 4.80 3.0 1 9 8 0.07 ## kidtrans 3 400 4.82 1.52 5 4.80 1.5 1 9 8 0.18 ## miniboxy 4 400 4.66 1.48 5 4.60 1.5 1 9 8 0.32 ## lthrbetr 5 400 4.25 1.52 4 4.24 1.5 1 9 8 0.23 ## secbiggr 6 400 5.23 1.51 5 5.26 1.5 1 9 8 -0.18 ## safeimpt 7 400 5.00 1.51 5 4.96 1.5 1 9 8 0.16 ## buyhghnd 8 400 5.28 1.54 5 5.31 1.5 1 9 8 -0.17 ## pricqual 9 400 4.96 1.52 5 4.95 1.5 1 9 8 0.06 ## prmsound 10 400 4.72 1.51 5 4.70 1.5 1 9 8 0.04 ## perfimpt 11 400 5.00 1.49 5 4.97 1.5 1 9 8 0.12 ## tkvacatn 12 400 4.44 1.53 5 4.45 1.5 1 9 8 0.16 ## noparkrm 13 400 4.58 1.52 4 4.50 1.5 1 9 8 0.34 ## homlrgst 14 400 5.49 1.50 5 5.46 1.5 1 9 8 -0.03 ## envrminr 15 400 5.53 1.52 6 5.56 1.5 1 9 8 -0.24 ## needbetw 16 400 4.94 1.55 5 4.91 1.5 1 9 8 0.12 ## suvcmpct 17 400 4.78 1.51 5 4.73 1.5 1 9 8 0.26 ## next2str 18 400 4.49 1.55 4 4.42 1.5 1 9 8 0.41 ## carefmny 19 400 5.22 1.53 5 5.24 1.5 1 9 8 0.02 ## shdcarpl 20 400 5.18 1.50 5 5.20 1.5 1 9 8 -0.13 ## imprtapp 21 400 4.56 1.50 5 4.57 1.5 1 9 8 0.10 ## lk4whldr 22 400 5.24 1.46 5 5.27 1.5 1 9 8 -0.17 ## kidsbulk 23 400 4.48 1.48 4 4.42 1.5 1 9 8 0.41 ## wntguzlr 24 400 5.06 1.51 5 5.06 1.5 1 9 8 -0.02 ## nordtrps 25 400 5.32 1.49 5 5.36 1.5 1 9 8 -0.19 ## stylclth 26 400 5.34 1.53 5 5.38 1.5 1 9 8 -0.18 ## strngwrn 27 400 5.04 1.50 5 5.06 1.5 1 9 8 -0.09 ## passnimp 28 400 5.62 1.51 6 5.63 1.5 1 9 8 -0.26 ## twoincom 29 400 4.87 1.56 5 4.87 1.5 1 9 8 0.01 ## nohummer 30 400 4.85 1.54 5 4.82 1.5 1 9 8 0.17 ## aftrschl 31 400 4.38 1.51 4 4.31 1.5 1 9 8 0.46 ## accesfun 32 400 4.64 1.52 5 4.65 1.5 1 9 8 0.01 ## age 33 400 40.06 8.57 40 40.07 8.9 19 60 41 -0.03 ## income 34 400 71.28 43.28 62 66.27 42.2 15 273 258 1.18 ## miles 35 400 18.04 4.86 18 17.93 4.5 7 32 25 0.20 ## numkids 36 400 1.22 1.08 1 1.10 1.5 0 4 4 0.74 ## female 37 400 0.54 0.50 1 0.55 0.0 0 1 1 -0.16 ## educ 38 400 2.81 0.93 3 2.88 1.5 1 4 3 -0.42 ## recycle 39 400 3.04 1.18 3 3.05 1.5 1 5 4 -0.04 ## kurtosis se ## subjnumb -1.21 5.78 ## mvliking -1.20 0.13 ## kidtrans -0.16 0.08 ## miniboxy -0.10 0.07 ## lthrbetr 0.07 0.08 ## secbiggr -0.13 0.08 ## safeimpt -0.05 0.08 ## buyhghnd -0.13 0.08 ## pricqual -0.12 0.08 ## prmsound -0.14 0.08 ## perfimpt 0.09 0.07 ## tkvacatn -0.06 0.08 ## noparkrm -0.17 0.08 ## homlrgst 0.15 0.08 ## envrminr 0.09 0.08 ## needbetw -0.20 0.08 ## suvcmpct -0.03 0.08 ## next2str -0.12 0.08 ## carefmny -0.35 0.08 ## shdcarpl 0.05 0.08 ## imprtapp 0.00 0.08 ## lk4whldr 0.30 0.07 ## kidsbulk 0.17 0.07 ## wntguzlr -0.06 0.08 ## nordtrps 0.08 0.07 ## stylclth -0.15 0.08 ## strngwrn 0.00 0.08 ## passnimp 0.19 0.08 ## twoincom -0.13 0.08 ## nohummer -0.15 0.08 ## aftrschl 0.11 0.08 ## accesfun -0.24 0.08 ## age -0.72 0.43 ## income 1.73 2.16 ## miles -0.32 0.24 ## numkids -0.09 0.05 ## female -1.98 0.02 ## educ -0.67 0.05 ## recycle -0.83 0.06 # scale data vars &lt;- scale(microvan[,2:32]) 2.1 Correlation Analysis 2.1.1 Pair-wise Scatter Plot 2.1.2 Correlation Matrix "],["exploratory-factor-analysis.html", "Chapter3 Exploratory Factor Analysis 3.1 Eigenvalues 3.2 Extract Principal Factors 3.3 Factor Loadings 3.4 Factor Scores", " Chapter3 Exploratory Factor Analysis 3.1 Eigenvalues # correlation analysis without mvliking cor&lt;- cor(vars[,-1]) EV = eigen(cor)$values EV_df = EV %&gt;% # convert to df as.data.frame()%&gt;% # name column dplyr::rename(., &quot;Eigen Value&quot; =&quot;.&quot;) %&gt;% # add a column &quot;Factor&quot; dplyr::mutate(Factor = seq(1,length(EV))) %&gt;% # order columns dplyr::select(Factor, everything()) %&gt;% # add a column &quot;Variance&quot; dplyr::mutate(`Variance %` = EV/length(EV) *100) %&gt;% dplyr::mutate(`Cumulative Variance % ` = cumsum(EV/length(EV))* 100) %&gt;% dplyr::mutate(., across(where(is.numeric), round, 1)) rmarkdown::paged_table(EV_df ) 3.1.1 Scree Plot psych::scree(cor, pc = TRUE, factors = FALSE) 3.1.2 Cumulative Percentages of Variance # Shares for the cumulative variance explained plot(cumsum(EV/length(EV)), type = &quot;o&quot;, # type of plot: &quot;o&quot; for points and lines &#39;overplotted&#39; col = &quot;darkblue&quot;, pch = 16, # plot symbol: 16 = filled circle cex = 1, # size of plot symbols xlab = &quot;Number of factors&quot;, # a title for the x axis ylab = &quot;Cumulative variance explained&quot;, # a title for the y axis lwd = 2) # line width abline(v = 5, lwd = 2, col = &quot;grey&quot;) # draw a vertical line at v = 3 3.1.3 Select Number of Factors using Kaiser Rule nFactor &lt;- length(which(EV &gt; 1)) print(paste0(nFactor, &quot; factors with Eigenvalue &gt; 1&quot; )) ## [1] &quot;5 factors with Eigenvalue &gt; 1&quot; 3.2 Extract Principal Factors EFA &lt;- psych::fa( r = cor, # number of factors nfactors = nFactor, # principal factor fm = &quot;pa&quot;, # maximizes the sum of the variance of the squared loadings rotate = &quot;varimax&quot; ) 3.3 Factor Loadings options(digits = 2) columnOrder &lt;- c(&quot;PA1&quot;, &quot;PA2&quot;, &quot;PA3&quot;, &quot;PA4&quot;,&quot;PA5&quot;,&quot;Communality&quot;,&quot;Uniqueness&quot;) EFA_loadings &lt;- data.frame(EFA$loadings[,] ) %&gt;% dplyr::mutate(Uniqueness = EFA$uniquenesses, Communality = EFA$communality) %&gt;% dplyr::mutate(., across(where(is.numeric), round, 2)) %&gt;% dplyr::select(columnOrder) %&gt;% arrange(., -Communality) # a function to highlight cells value &gt; 0.5 or &lt; -0.5 colorCell &lt;- function(x, n, nmax){ cell_spec(x, background = ifelse(is.na(as.numeric(x)), &quot;white&quot;, ifelse(n == nmax | n == 1, &quot;white&quot;, ifelse(abs(x) &gt;=0.5, &quot;lightgreen&quot;, ifelse(x &lt; 0.5, &quot;white&quot;, &quot;white&quot;))))) } # show table in rMarkdown EFA_loading_table = EFA_loadings %&gt;% mutate_all(funs(colorCell(., n = row_number(), nmax = n()))) %&gt;% kable(escape = FALSE) %&gt;% kable_styling(bootstrap_options = &quot;striped&quot;, full_width = F, position = &quot;center&quot;) paged_table(EFA_loadings) 3.4 Factor Scores # extract rotated factor scores EFA.scores = factor.scores(vars[,-1], unclass(EFA$loadings))$scores "],["regression-with-5-factors.html", "Chapter4 Regression with 5 Factors", " Chapter4 Regression with 5 Factors # data for regression EFA_with_score &lt;- cbind.data.frame(vars[,2], EFA.scores) %&gt;% dplyr::rename(., mvliking = &quot;vars[, 2]&quot; ) EFA_model &lt;- lm(mvliking ~ PA1 + PA2 +PA3+PA4 +PA5, data=EFA_with_score ) summary(EFA_model) ## ## Call: ## lm(formula = mvliking ~ PA1 + PA2 + PA3 + PA4 + PA5, data = EFA_with_score) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.7272 -0.0848 -0.0030 0.0900 0.9830 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.05e-16 7.80e-03 0.00 1.0000 ## PA1 1.22e-01 7.81e-03 15.64 &lt;2e-16 *** ## PA2 2.29e-03 7.81e-03 0.29 0.7694 ## PA3 9.80e-01 7.81e-03 125.54 &lt;2e-16 *** ## PA4 3.03e-03 7.81e-03 0.39 0.6983 ## PA5 -2.42e-02 7.81e-03 -3.10 0.0021 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.16 on 394 degrees of freedom ## Multiple R-squared: 0.976, Adjusted R-squared: 0.976 ## F-statistic: 3.2e+03 on 5 and 394 DF, p-value: &lt;2e-16 # merge with all previous data #EFA_merge_data &lt;- cbind(microvan, EFA.scores) #head(EFA_merge_data) "],["regression-with-3-factors.html", "Chapter5 Regression with 3 Factors", " Chapter5 Regression with 3 Factors # data for regression EFA_model2 &lt;- lm(mvliking ~ PA1 + PA3 + PA5, data=EFA_with_score ) summary(EFA_model2) ## ## Call: ## lm(formula = mvliking ~ PA1 + PA3 + PA5, data = EFA_with_score) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.7268 -0.0841 -0.0035 0.0910 0.9816 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.06e-16 7.78e-03 0.00 1.000 ## PA1 1.22e-01 7.79e-03 15.68 &lt;2e-16 *** ## PA3 9.80e-01 7.79e-03 125.82 &lt;2e-16 *** ## PA5 -2.42e-02 7.79e-03 -3.11 0.002 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.16 on 396 degrees of freedom ## Multiple R-squared: 0.976, Adjusted R-squared: 0.976 ## F-statistic: 5.36e+03 on 3 and 396 DF, p-value: &lt;2e-16 # merge with all previous data #EFA_merge_data &lt;- cbind(microvan, EFA.scores) #head(EFA_merge_data) "],["clustering.html", "Chapter6 Clustering", " Chapter6 Clustering EFA_feature = EFA_with_score %&gt;% dplyr::select(PA1, PA2,PA4) # Run a cluster analysis on a distance matrix and using the Ward method c&lt;- hclust(dist(EFA_feature), method=&quot;ward.D2&quot;) # Dendrogram library(dendextend) plot(set(as.dendrogram(c), &quot;branches_k_color&quot;, # to highlight the cluster solution with a color k = 3), ylab = &quot;Distance&quot;, main = &quot;Dendrogram&quot;, cex = 0.2) EFA_kmeans &lt;- kmeans(EFA_feature , centers = 3) EFA_kmeans ## K-means clustering with 3 clusters of sizes 157, 125, 118 ## ## Cluster means: ## PA1 PA2 PA4 ## 1 -1.01 -0.038 -0.064 ## 2 0.61 1.117 0.069 ## 3 0.70 -1.133 0.013 ## ## Clustering vector: ## [1] 1 2 3 2 3 3 2 1 3 1 1 3 1 3 2 2 2 2 1 1 1 1 2 1 1 3 1 2 1 3 1 3 1 3 ## [35] 3 3 3 2 3 3 2 1 1 1 1 2 1 3 3 1 3 3 1 3 1 1 2 3 2 1 3 1 1 1 2 1 1 2 ## [69] 2 3 2 1 1 3 3 1 1 3 1 2 3 1 1 1 3 1 1 3 1 1 3 2 1 2 3 3 3 1 1 1 1 3 ## [103] 1 1 1 2 1 2 3 3 2 2 1 2 1 2 2 1 1 1 3 1 2 3 2 1 1 1 2 1 3 3 3 2 2 3 ## [137] 3 3 2 2 2 3 1 1 2 1 2 2 1 1 3 1 2 2 3 1 2 1 3 2 2 3 2 1 1 1 3 3 1 3 ## [171] 2 1 1 2 3 2 3 2 2 3 1 1 2 3 2 1 3 1 1 3 2 3 1 1 2 1 1 2 2 1 2 1 3 1 ## [205] 3 3 1 3 1 2 3 2 1 3 3 1 3 2 1 3 2 1 3 1 3 1 2 1 2 1 1 3 3 3 3 3 3 2 ## [239] 1 2 1 2 2 1 3 1 2 3 1 1 3 1 2 2 2 1 3 1 1 2 3 3 3 2 3 2 1 1 2 1 3 1 ## [273] 3 1 2 3 2 1 1 2 3 2 2 2 3 1 1 3 2 2 2 1 1 2 2 1 3 2 2 3 2 1 3 2 1 3 ## [307] 2 2 3 1 1 1 1 2 1 2 1 2 2 3 1 2 1 2 1 2 1 1 1 2 2 1 3 2 2 1 1 1 2 1 ## [341] 3 3 3 3 3 2 1 2 3 1 2 1 3 1 3 2 2 2 3 2 2 2 1 2 1 1 1 2 2 1 3 3 2 2 ## [375] 2 3 2 1 3 3 2 3 3 2 1 1 3 1 3 1 3 1 2 3 3 1 1 2 2 1 ## ## Within cluster sum of squares by cluster: ## [1] 255 207 162 ## (between_SS / total_SS = 47.9 %) ## ## Available components: ## ## [1] &quot;cluster&quot; &quot;centers&quot; &quot;totss&quot; &quot;withinss&quot; ## [5] &quot;tot.withinss&quot; &quot;betweenss&quot; &quot;size&quot; &quot;iter&quot; ## [9] &quot;ifault&quot; # factor plot fviz_cluster(EFA_kmeans, data = EFA_feature) + theme_bw() "],["discussion.html", "Chapter7 Discussion", " Chapter7 Discussion "],["references.html", "References", " References Reducing Dimensionality Latent Dimensions of Customer Perceptions Hierarchical Clustering Preference Segmentation Preference Segmentation - Validation "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
